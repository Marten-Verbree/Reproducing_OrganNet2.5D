{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rebuilding OrganNet2.5D.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marten-Verbree/Reproducing_OrganNet2.5D/blob/main/Rebuilding_OrganNet2_5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "it9k1HSGoeXt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be821e46-64ac-4ccf-d356-23eff2164398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: simpleitk==2.0.0 in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: MedPy in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from MedPy) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from MedPy) (1.19.5)\n",
            "Requirement already satisfied: SimpleITK>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from MedPy) (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "#imports\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummary import summary\n",
        "!pip install simpleitk==\"2.0.0\"\n",
        "!pip install MedPy\n",
        "from medpy.io import load\n",
        "from scipy.ndimage import zoom"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "oCLVGsZPVNZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = 'rebuildingorgannet25d'\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import uuid\n",
        "bucket_name = 'rebuildingbucket'"
      ],
      "metadata": {
        "id": "O8AJH1HLQmDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project {project_id}"
      ],
      "metadata": {
        "id": "HrpT8lpyHi1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89f7d001-c650-4924-ff06-d14e87d9daa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil -m cp -r gs://{bucket_name}/_data /content/_data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LYJXjvhHlUZ",
        "outputId": "670f6e15-f3f8-4747-c716-28430b1cd46f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0555/img_resampled_0522c0555.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0555/mask_resampled_0522c0555.mha...\n",
            "/ [0 files][    0.0 B/  1.5 GiB]                                                \r/ [0/137 files][    0.0 B/  2.1 GiB]   0% Done                                  \rCopying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0555/voxelinfo.json...\n",
            "/ [0/137 files][    0.0 B/  2.1 GiB]   0% Done                                  \rCopying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0576/img_resampled_0522c0576.mha...\n",
            "/ [0/137 files][    0.0 B/  2.1 GiB]   0% Done                                  \rCopying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0576/mask_resampled_0522c0576.mha...\n",
            "/ [0/137 files][    0.0 B/  2.1 GiB]   0% Done                                  \rCopying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0576/voxelinfo.json...\n",
            "/ [0/137 files][    0.0 B/  2.1 GiB]   0% Done                                  \rCopying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0598/mask_resampled_0522c0598.mha...\n",
            "/ [0/137 files][    0.0 B/  2.1 GiB]   0% Done                                  \rCopying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0598/img_resampled_0522c0598.mha...\n",
            "/ [0/137 files][    0.0 B/  2.1 GiB]   0% Done                                  \rCopying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0598/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0659/img_resampled_0522c0659.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0659/mask_resampled_0522c0659.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0659/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0661/img_resampled_0522c0661.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0661/mask_resampled_0522c0661.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0661/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0667/img_resampled_0522c0667.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0667/mask_resampled_0522c0667.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0667/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0669/img_resampled_0522c0669.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0669/mask_resampled_0522c0669.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0669/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0708/img_resampled_0522c0708.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0708/mask_resampled_0522c0708.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0708/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0727/img_resampled_0522c0727.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0727/mask_resampled_0522c0727.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0727/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0746/img_resampled_0522c0746.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0746/mask_resampled_0522c0746.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/0522c0746/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/img.csv...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/img_resampled.csv...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/mask.csv...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/mask_resampled.csv...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0001/img_resampled_0522c0001.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0001/mask_resampled_0522c0001.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0001/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0002/img_resampled_0522c0002.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0002/mask_resampled_0522c0002.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0002/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0003/img_resampled_0522c0003.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0003/mask_resampled_0522c0003.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0003/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0009/img_resampled_0522c0009.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0009/mask_resampled_0522c0009.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0009/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0013/img_resampled_0522c0013.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0013/mask_resampled_0522c0013.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0013/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0014/img_resampled_0522c0014.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0014/mask_resampled_0522c0014.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0014/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0017/img_resampled_0522c0017.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0017/mask_resampled_0522c0017.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0017/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0057/img_resampled_0522c0057.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0057/mask_resampled_0522c0057.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0057/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0070/img_resampled_0522c0070.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0070/mask_resampled_0522c0070.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0070/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0077/img_resampled_0522c0077.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0077/mask_resampled_0522c0077.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0077/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0079/img_resampled_0522c0079.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0079/mask_resampled_0522c0079.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0079/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0081/img_resampled_0522c0081.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0081/mask_resampled_0522c0081.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0081/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0125/img_resampled_0522c0125.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0125/mask_resampled_0522c0125.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0125/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0132/img_resampled_0522c0132.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0132/mask_resampled_0522c0132.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0132/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0147/img_resampled_0522c0147.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0147/mask_resampled_0522c0147.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0147/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0149/img_resampled_0522c0149.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0149/mask_resampled_0522c0149.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0149/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0159/img_resampled_0522c0159.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0159/mask_resampled_0522c0159.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0159/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0161/img_resampled_0522c0161.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0161/mask_resampled_0522c0161.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0161/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0190/img_resampled_0522c0190.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0190/mask_resampled_0522c0190.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0190/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0195/img_resampled_0522c0195.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0195/mask_resampled_0522c0195.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0195/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0226/img_resampled_0522c0226.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0226/mask_resampled_0522c0226.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0226/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0248/img_resampled_0522c0248.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0248/mask_resampled_0522c0248.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0248/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0251/img_resampled_0522c0251.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0251/mask_resampled_0522c0251.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0251/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0253/img_resampled_0522c0253.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0253/mask_resampled_0522c0253.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0253/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0328/img_resampled_0522c0328.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0328/mask_resampled_0522c0328.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0328/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0329/img_resampled_0522c0329.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0329/mask_resampled_0522c0329.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0329/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0330/img_resampled_0522c0330.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0330/mask_resampled_0522c0330.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0330/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0427/img_resampled_0522c0427.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0427/mask_resampled_0522c0427.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0427/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0433/img_resampled_0522c0433.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0433/mask_resampled_0522c0433.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0433/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0441/img_resampled_0522c0441.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0441/mask_resampled_0522c0441.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0441/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0455/img_resampled_0522c0455.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0455/mask_resampled_0522c0455.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0455/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0457/img_resampled_0522c0457.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0457/mask_resampled_0522c0457.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0457/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0479/img_resampled_0522c0479.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0479/mask_resampled_0522c0479.mha...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/0522c0479/voxelinfo.json...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/img.csv...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/img_resampled.csv...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/mask.csv...\n",
            "Copying gs://rebuildingbucket/_data/HaN_MICCAI2015/processed/train/data_3D/mask_resampled.csv...\n",
            "| [137/137 files][  2.1 GiB/  2.1 GiB] 100% Done  67.9 MiB/s ETA 00:00:00       \n",
            "Operation completed over 137 objects/2.1 GiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install zipfile36\n",
        "# import zipfile\n",
        "# with zipfile.ZipFile(\"file.zip\",\"r\") as zip_ref:\n",
        "#     zip_ref.extractall(\"targetdir\")"
      ],
      "metadata": {
        "id": "J-t4kB7xpKp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loader\n",
        "\n",
        "imgs = \"/content/_data/_data/HaN_MICCAI2015/processed/train/data_3D/img_resampled.csv\"\n",
        "msks = \"/content/_data/_data/HaN_MICCAI2015/processed/train/data_3D/mask_resampled.csv\"\n",
        "t_imgs = \"/content/_data/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/img_resampled.csv\"\n",
        "t_msks = \"/content/_data/_data/HaN_MICCAI2015/processed/test_offsite/data_3D/mask_resampled.csv\"\n",
        "shape = (1, 800, 800, 232)\n",
        "\n",
        "class TorchDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, x_data, y_data, number_of_slices = 64):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x_data (xarray): if test, 2000 by 6. if training, 8000 by 6 array.\n",
        "            y_data (xarray): if test, 2000 by 3, if training, 8000 by 3 array.\n",
        "        \"\"\"\n",
        "        imges = open(x_data, \"r\")\n",
        "        images = imges.readlines()\n",
        "        self.x_data = [x[:-1] for x in images]\n",
        "        masks = open(y_data, \"r\")\n",
        "        masks_read = masks.readlines()\n",
        "        self.number_of_slices = number_of_slices\n",
        "        self.y_data = [x[:-1] for x in masks_read]\n",
        "        imges.close()\n",
        "        masks.close()\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            idx (int or 0D tensor): index of\n",
        "        \"\"\"\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        x,_ = load(self.x_data[idx])\n",
        "        y,_ = load(self.y_data[idx])\n",
        "        \n",
        "        # Normalization\n",
        "        mean = x.mean()\n",
        "        std = x.std()\n",
        "        x = (x-mean)/std\n",
        "\n",
        "        # Resizing\n",
        "        x = np.copy(x)\n",
        "        x.resize(shape)  \n",
        "        y = np.copy(y)\n",
        "        y.resize(shape)\n",
        "\n",
        "        # print('x:', slice_idx_x)\n",
        "        # print('y:', slice_idx_y)\n",
        "        \n",
        "        x = zoom(x, (1, 0.5, 0.5, 1))\n",
        "        y = zoom(y, (1, 0.5, 0.5, 1))\n",
        "        \n",
        "        # print('shape x: ', x_datapoint.shape)\n",
        "        # print('shape y: ', y_datapoint.shape)\n",
        "        sample = (x,y)\n",
        "        return sample\n",
        "training_dataset = TorchDataset(imgs, msks)\n",
        "test_dataset = TorchDataset(t_imgs, t_msks)\n",
        "\n"
      ],
      "metadata": {
        "id": "kxycZCOKuUAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a dataloader\n",
        "train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "zuh9DbLL4J8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock2D(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(ConvBlock2D, self).__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels,out_channels, kernel_size=(3,3,1), padding='same') #unsure whether padding is used, assuming that it is\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.batchnorm = nn.BatchNorm3d(out_channels)\n",
        "    self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=(3,3,1), padding='same') #not clearly mentioned in paper that it is out_channels to out_channels\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.batchnorm2 = nn.BatchNorm3d(out_channels)\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.batchnorm(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.batchnorm2(x)\n",
        "    x = self.relu2(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "shZhKkO7PE26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock3DResse(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(ConvBlock3DResse, self).__init__()\n",
        "    self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, padding='same')\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.batchnorm1 = nn.BatchNorm3d(out_channels)\n",
        "    self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, padding='same')\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.batchnorm2 = nn.BatchNorm3d(out_channels)\n",
        "    self.globalpool = nn.AdaptiveAvgPool3d(output_size=1)\n",
        "    self.flatten1 = nn.Flatten() #might have to change start_dim\n",
        "    self.linear1 = nn.Linear(1, 1)\n",
        "    self.relu3 = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(1, 1)\n",
        "    self.sigmoid1 = nn.Sigmoid()\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.batchnorm1(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.batchnorm2(x)\n",
        "    x = self.relu2(x)\n",
        "    x1 = self.globalpool(x)\n",
        "    x1 = self.linear1(x1)\n",
        "    x1 = self.relu3(x1)\n",
        "    x1 = self.linear2(x1)\n",
        "    x1 = self.sigmoid1(x1)\n",
        "    return x1*x + x\n",
        "\n"
      ],
      "metadata": {
        "id": "Knbx2NoYStUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridDilatedConv3DResse(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(HybridDilatedConv3DResse, self).__init__()\n",
        "    #different dilation rates? but how different?\n",
        "    self.hdc = nn.Conv3d(in_channels, out_channels, kernel_size=3, padding='same', dilation = 2)\n",
        "    self.batchnorm1 = nn.BatchNorm3d(out_channels)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.globalpool = nn.AdaptiveAvgPool3d(output_size=1)\n",
        "    self.flatten1 = nn.Flatten() #might not be necessary, let's see what the output is of the globalpooling. I suppose this is already flattened (output_size=1).\n",
        "    self.linear1 = nn.Linear(1, 1)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(1, 1)\n",
        "    self.sigmoid1 = nn.Sigmoid()\n",
        "  def forward(self, x):\n",
        "    x = self.hdc(x)\n",
        "    x = self.batchnorm1(x)\n",
        "    x = self.relu1(x)\n",
        "    x1 = self.globalpool(x)\n",
        "    x1 = self.linear1(x1)\n",
        "    x1 = self.relu2(x1)\n",
        "    x1 = self.linear2(x1)\n",
        "    x1 = self.sigmoid1(x1)\n",
        "    return x1*x + x\n"
      ],
      "metadata": {
        "id": "ufFi3wG9arSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv3Dfine(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(Conv3Dfine, self).__init__()\n",
        "    self.conv3D = nn.Conv3d(in_channels, out_channels, kernel_size=1, padding='same')\n",
        "    self.relu = nn.ReLU()\n",
        "    self.batchnorm = nn.BatchNorm3d(out_channels)\n",
        "  def forward(self, x):\n",
        "    x = self.conv3D(x)\n",
        "    x = self.batchnorm(x)\n",
        "    x = self.relu(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "XqfraWP7MD_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "critique: Very much uncertain what type of pooling is done, it is only specified for one layer. We asssume this to be constant. Based on advice of TA we do maxpooling, but it's bad that we have to rely on advice instead of just the paper. Also, it would be nice if strides and padding would be mentioned. "
      ],
      "metadata": {
        "id": "MkLv4zolKwuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)"
      ],
      "metadata": {
        "id": "iJS2WjQ7lR5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60752e4c-6893-4376-fe6c-615a5ffab2db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f36b630cb90>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TorchCNN(nn.Module):\n",
        "  def __init__(self, in_channels, hidden_channels, out_features):\n",
        "    super(TorchCNN, self).__init__()\n",
        "    self.conv2D1 = ConvBlock2D(in_channels, hidden_channels[0])\n",
        "    \n",
        "    self.pool1 = nn.MaxPool3d(kernel_size=(2,2,1),stride=(2,2,1)) \n",
        "    self.conv3D_coarse1 = ConvBlock3DResse(hidden_channels[0],hidden_channels[1])\n",
        "    self.pool2 = nn.MaxPool3d(kernel_size=2, stride =2)\n",
        "    self.conv3D_coarse2 = ConvBlock3DResse(hidden_channels[1],hidden_channels[2])\n",
        "    self.hdc1 = HybridDilatedConv3DResse(hidden_channels[2], hidden_channels[3])\n",
        "    self.hdc2 = HybridDilatedConv3DResse(hidden_channels[3], hidden_channels[4])\n",
        "    self.conv3D_fine1 = Conv3Dfine(hidden_channels[4], hidden_channels[3])\n",
        "    self.hdc3 = HybridDilatedConv3DResse(hidden_channels[4], hidden_channels[3])\n",
        "    self.conv3D_fine2 = Conv3Dfine(hidden_channels[3], hidden_channels[2])\n",
        "    self.conv3D_coarse3 = ConvBlock3DResse(hidden_channels[3],hidden_channels[2])\n",
        "    self.transpose1 = nn.ConvTranspose3d(hidden_channels[2], hidden_channels[1], kernel_size = 2, stride=2)\n",
        "    self.conv3D_coarse4 = ConvBlock3DResse(hidden_channels[2], hidden_channels[1])\n",
        "    self.transpose2 = nn.ConvTranspose3d(hidden_channels[1], hidden_channels[0], kernel_size = (2,2,1), stride=(2,2,1))\n",
        "    self.conv2D2 = ConvBlock2D(hidden_channels[1], hidden_channels[1])\n",
        "    self.conv3D_fine3 = Conv3Dfine(hidden_channels[1], out_features)\n",
        "  def forward(self, x):\n",
        "    x1 = self.conv2D1(x)\n",
        "    x2 = self.pool1(x1)\n",
        "    x2 = self.conv3D_coarse1(x2)\n",
        "    x3 = self.pool2(x2)\n",
        "    x3 = self.conv3D_coarse2(x3)\n",
        "    x4 = self.hdc1(x3)\n",
        "    x5 = self.hdc2(x4)\n",
        "    x5 = self.conv3D_fine1(x5)    \n",
        "    x5 = torch.cat((x4, x5), dim=1)\n",
        "    x5 = self.hdc3(x5)\n",
        "    x5 = self.conv3D_fine2(x5)\n",
        "    x5 = torch.cat((x3, x5), dim=1)\n",
        "    x5 = self.conv3D_coarse3(x5)\n",
        "    x5 = self.transpose1(x5)\n",
        "    x5 = torch.cat((x2,x5), dim =1)\n",
        "    x5 = self.conv3D_coarse4(x5)\n",
        "    x5 = self.transpose2(x5)\n",
        "    x5 = torch.cat((x1,x5), dim = 1)\n",
        "    x5 = self.conv2D2(x5)\n",
        "    x5 = self.conv3D_fine3(x5)\n",
        "    return x5\n",
        "    #Global average pooling is not implemented, apparently this is an alternative: \n",
        "#   #x = torch.randn(16, 14, 14)\n",
        "    # out = F.adaptive_max_pool2d(x.unsqueeze(0), output_size=1)"
      ],
      "metadata": {
        "id": "PsgB1--gqSGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in_channels = 1\n",
        "# hidden_channels = [16, 32, 64, 128, 256]\n",
        "# out_channels = 10 # for Miccai data set\n",
        "# CNN = TorchCNN(in_channels, hidden_channels, out_channels)\n",
        "# x = torch.randn((1, 1, 100, 100, 232))\n",
        "# out = CNN(x)\n",
        "# print(out.shape)"
      ],
      "metadata": {
        "id": "hV_3jLu9b3UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coef(y_true, y_pred, epsilon=1e-6, testing = False):\n",
        "    # Computes the SÃ¸rensen-dice score coefficien(DSC).\n",
        "    #   DSC = (2*(|X&Y|)\\(|X| + |Y|)\n",
        "    #      = 2*sum(|A*B|)/(sum(A^2 + sum(B^2)\n",
        "    #    ref: https://github.com/shalabh147/Brain-Tumor-Segmentation-and-Survival-Prediction-using-Deep-Neural-Networks/blob/master/utils.py\n",
        "    #   ref: https://gist.github.com/jeremyjordan/9ea3032a32909f71dd2ab35fe3bacc08#file-soft_dice_loss-py\n",
        "\n",
        "    #Args:\n",
        "    #  :param y_true: is a tensor [H, W, D, L] with the ground truth of the OAR\n",
        "    #  :param y_pred: is a tensor [H, W, D, L] with the predicted area of the OAR\n",
        "    #  :param epsilon: Used for numerical stability to avoid divide by zeros.\n",
        "    \n",
        "    dice_labels = torch.zeros(y_pred.shape)\n",
        "\n",
        "    y_pred_label = y_pred\n",
        "    y_true_label = y_true\n",
        "    if torch.sum(y_true_label) > 0 and not testing:\n",
        "          dice_numerator = 2 * torch.sum(y_true_label * y_pred_label)\n",
        "          dice_denominator = 10*torch.sum(y_true_label + y_pred_label) + epsilon\n",
        "          dice_score = dice_numerator/dice_denominator\n",
        "          # print('dice score:', dice_score)\n",
        "    elif torch.sum(y_true_label)>0 and testing:\n",
        "          dice_numerator = 2 * torch.sum(y_true_label * y_pred_label, dim = (2,3,4))\n",
        "          dice_denominator = torch.sum(y_true_label + y_pred_label, dim = (2,3,4)) + epsilon\n",
        "          dice_score = dice_numerator/dice_denominator\n",
        "          # print('dice scores: ', dice_score)\n",
        "    elif testing:\n",
        "          dice_score = torch.zeros(10)\n",
        "    else:\n",
        "          dice_score = 0\n",
        "    return dice_score\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    dice_coef1 = dice_coef(y_true, y_pred)\n",
        "    # print('dice_coef loss:', 1 - dice_coef1)\n",
        "    return 1 - dice_coef1\n",
        "\n",
        "\n",
        "def focal_loss(y_true, y_pred,  epsilon =1e-6):\n",
        "    \"\"\" Computes the focal loss.\n",
        "            FL(p_t) = mean(-alpha(1-p_t^gamma)* y *ln(p_t)\n",
        "            Notice: y_pred is probability after using softmax\n",
        "            ref: https://arxiv.org/pdf/2109.12634.pdf\n",
        "\n",
        "        Args:\n",
        "            :param y_true: is a tensor [H, W, D, L] with the ground truth of the OAR\n",
        "            :param y_pred: is a tensor [H, W, D, L] with the predicted area of the OAR\n",
        "            :param epsilon: Used for numerical stability to avoid divide by zeros\n",
        "            :param gamma: Focal Tversky loss' focal parameter controls degree of down-weighting of easy examples, by default 2.0\n",
        "            :param ALPHA: assigned weights according to Chen et al. (2021)\n",
        "        \"\"\"\n",
        "\n",
        "    ALPHA = torch.tensor([0.5, 1.0, 4.0, 1.0, 4.0, 4.0, 1.0, 1.0, 3.0, 3.0]) \n",
        "    GAMMA = 2\n",
        "\n",
        "    loss_labels = torch.zeros((ALPHA.shape[0],100,100,232))\n",
        "\n",
        "    for i in range(y_pred.shape[1]):\n",
        "\n",
        "        y_pred_label = y_pred[:, i]\n",
        "        y_true_label =  y_true[:, i]\n",
        "\n",
        "        y_pred_clamp = torch.softmax(y_pred_label, dim=1) \n",
        "        # print('y_pred_clamp', y_pred_clamp)\n",
        "        cross_entropy = -y_true_label * torch.log(y_pred_clamp + epsilon)\n",
        "        # print('cross_entropy', cross_entropy)\n",
        "        back_ce = torch.pow(1 - y_pred_clamp, GAMMA) * cross_entropy\n",
        "        \n",
        "        focal_loss_label = torch.mul(ALPHA[i], back_ce)\n",
        "        # print('focal_loss_label', focal_loss_label)\n",
        "\n",
        "        loss_labels[i] = focal_loss_label\n",
        "\n",
        "    loss = torch.mean(loss_labels)\n",
        "    # print('focal loss:', loss)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def final_loss(y_true, y_pred):\n",
        "    return focal_loss(y_true, y_pred) + dice_coef_loss(y_true, y_pred)\n"
      ],
      "metadata": {
        "id": "88Y64cchvViq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def try_gpu():\n",
        "    \"\"\"\n",
        "    If GPU is available, return torch.device as cuda:0; else return torch.device\n",
        "    as cpu.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda:0')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "    return device"
      ],
      "metadata": {
        "id": "HBiAG2n2tmq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training parameters\n",
        "learning_rate = 0.001\n",
        "epochs = 6\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "in_channels = 1\n",
        "hidden_channels = [16, 32, 64, 128, 256]\n",
        "out_channels = 10 # for Miccai data set\n",
        "CNN = TorchCNN(in_channels, hidden_channels, out_channels)\n",
        "CNN = CNN.float()"
      ],
      "metadata": {
        "id": "Urss0t8WlaFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(CNN.parameters(), lr = learning_rate) # check what optimizer is used in paper\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
        "device = try_gpu()\n",
        "CNN.train()\n",
        "CNN.to(device)\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # Network in training mode and to device\n",
        "    \n",
        "\n",
        "    train_dsc = torch.zeros((1,10)).to(device)\n",
        "    # Training loop\n",
        "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "        \n",
        "        # Set to same device\n",
        "        x_batch, y_batch = x_batch.to(device).float(), y_batch.to(device).float()\n",
        "        for idx_h in range(4):\n",
        "          for idx_w in range(4):\n",
        "            x_batch_mod = x_batch[:,:,idx_h*100:100*idx_h+100, idx_w*100:100*idx_w+100,:]\n",
        "            y_batch_mod = y_batch[:,:,idx_h*100:100*idx_h+100, idx_w*100:100*idx_w+100,:]\n",
        "        # Set the gradients to zero\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Perform forward pass\n",
        "            y_pred = CNN(x_batch_mod)\n",
        "            del x_batch_mod #emptying memory\n",
        "            # Compute the loss\n",
        "            loss = final_loss(y_pred, y_batch_mod)\n",
        "            train_dsc += 100/len(train_loader)*dice_coef(y_pred, y_batch_mod, testing=True).to(device) #change to dice score\n",
        "            # print(train_dsc)\n",
        "            del y_batch_mod\n",
        "            del y_pred\n",
        "            # Backward computation and update\n",
        "            loss.backward()\n",
        "            train_losses.append(loss.detach().item())\n",
        "            del loss\n",
        "            optimizer.step()\n",
        "        # if i % 32 == 0:\n",
        "        #     print('loss:', train_losses[64*epoch+i])\n",
        "        del x_batch\n",
        "        del y_batch\n",
        "        \n",
        "    \n",
        "\n",
        "    scheduler.step()\n",
        "    \n",
        "    \n",
        "    # Development of performance\n",
        "    train_accs.append(train_dsc.detach().tolist())\n",
        "    CNN.eval()\n",
        "    test_dsc = torch.zeros((1,10)).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i, (x_batch, y_batch) in enumerate(test_loader):\n",
        "            x_batch, y_batch = x_batch.to(device).float(), y_batch.to(device).float()\n",
        "            for idx_h in range(4):\n",
        "              for idx_w in range(4):\n",
        "                x_batch_mod = x_batch[:,:,idx_h*100:100*idx_h+100, idx_w*100:100*idx_w+100,:]\n",
        "                y_batch_mod = y_batch[:,:,idx_h*100:100*idx_h+100, idx_w*100:100*idx_w+100,:]\n",
        "                y_pred = CNN(x_batch_mod)\n",
        "                del x_batch_mod\n",
        "                test_dsc +=  torch.mul(100/len(test_loader), dice_coef(y_pred, y_batch_mod, testing=True)).to(device) #change to dice score\n",
        "                del y_batch_mod\n",
        "                del y_pred\n",
        "            del y_batch\n",
        "            del x_batch\n",
        "            \n",
        "    test_accs.append(test_dsc.detach().tolist())\n",
        "    print('Dice score of test set: {:.00f}%'.format(test_dsc.mean()))\n",
        "    del test_dsc\n",
        "    # Print performance\n",
        "    print('Epoch: {:.0f}'.format(epoch+1))\n",
        "    print('Dice score of train set: {:.00f}%'.format(train_dsc.mean()))\n",
        "    del train_dsc\n",
        "    print('loss of train set:', train_losses[-1])\n",
        "    print('')"
      ],
      "metadata": {
        "id": "mYYhMX_vpdDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c9a3498-e6c8-4abd-e70b-cde88f0625dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dice score of test set: 28%\n",
            "Epoch: 1\n",
            "Dice score of train set: 1015%\n",
            "loss of train set: 0.9977800846099854\n",
            "\n",
            "Dice score of test set: 0%\n",
            "Epoch: 2\n",
            "Dice score of train set: 195%\n",
            "loss of train set: 1.0\n",
            "\n",
            "Dice score of test set: 0%\n",
            "Epoch: 3\n",
            "Dice score of train set: 0%\n",
            "loss of train set: 1.0\n",
            "\n",
            "Dice score of test set: 0%\n",
            "Epoch: 4\n",
            "Dice score of train set: 0%\n",
            "loss of train set: 1.0\n",
            "\n",
            "Dice score of test set: 0%\n",
            "Epoch: 5\n",
            "Dice score of train set: 0%\n",
            "loss of train set: 1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training cycle:\n",
        "# print('Epoch: {:.0f}'.format(epoch+1))\n",
        "# print('Accuracy of train set: {:.00f}%'.format(train_dsc))\n",
        "# # print('Accuracy of test set: {:.00f}%'.format(test_dsc))\n",
        "# print('')"
      ],
      "metadata": {
        "id": "PR4BEfSLPNqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"/content/\"\n",
        "torch.save(CNN.state_dict(), PATH + 'model_params')\n",
        "list_ = train_losses\n",
        "torch.save(list_, PATH +'training_loss')\n",
        "torch.save(train_accs/16, PATH + 'training_dscs')\n",
        "torch.save(test_accs/16, PATH + 'test_dscs')"
      ],
      "metadata": {
        "id": "HSwTJJk17Tg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pocxnSWnSEpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_channels = 1\n",
        "hidden_channels = [16, 32, 64, 128, 256]\n",
        "out_channels = 10 # for Miccai data set\n",
        "Test_CNN = TorchCNN(in_channels, hidden_channels, out_channels)\n",
        "Test_CNN = CNN.float()\n",
        "Test_CNN.load_state_dict(torch.load(PATH))\n",
        "Test_CNN.eval()\n",
        "test_dsc = 0\n",
        "with torch.no_grad():\n",
        "  for i, (x_batch, y_batch) in enumerate(test_loader):\n",
        "    x_batch, y_batch = x_batch.to(device).float(), y_batch.to(device).float()\n",
        "        \n",
        "    y_pred = Test_CNN(x_batch)\n",
        "    del x_batch\n",
        "    test_dsc += 100/len(test_loader)*dice_coef(y_pred, y_batch) #change to dice score\n",
        "    del y_batch\n",
        "    del y_pred\n",
        "print('Dice score of test set: {:.00f}%'.format(test_dsc))"
      ],
      "metadata": {
        "id": "AqEOIyAq-PIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = torch.load('/content/drive/MyDrive/_model/training_loss')\n",
        "print(train_losses)"
      ],
      "metadata": {
        "id": "mh7L05jsPnZL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}